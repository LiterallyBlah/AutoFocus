# AutoFocus

**AutoFocus** is a companion tool designed to work alongside [AutoRecon](https://github.com/Tib3rius/AutoRecon), providing enhanced analysis of reconnaissance data through AI-powered processing. AutoFocus is built to streamline the initial engagement phase of penetration testing by focusing on detecting version numbers, identifying points of interest, and highlighting vulnerabilities with minimal manual effort.

## Features

- **Automatic File Enumeration**: AutoFocus recursively enumerates all files generated by AutoRecon, which are organised into target directories by IP address.

- **AI-Powered Chunked Data Processing**: AutoFocus uses a local Large Language Model (LLM) to analyse files in manageable chunks. This allows for efficient handling of large datasets, especially those generated during extensive reconnaissance efforts.

- **Task-Based Analysis**: The tool utilises configurable tasks, specified via YAML or JSON, to define the specific data points that need analysis, such as version numbers, points of interest, and vulnerabilities. Each task is designed to target specific objectives in the data.

- **Structured JSON Output**: All findings are saved in a structured JSON format, with results organised by IP address and task type. This structured output allows for easy integration into other tools or streamlined reporting.

- **Progress Updates and Error Handling**: Provides continuous progress updates during the analysis process and includes error-handling mechanisms to ensure reliable operation during data processing.

- **Extensible and Customisable**: AutoFocus is flexible and can easily be adapted to meet the needs of different reconnaissance scenarios. Users can add or modify tasks by editing the `tasks.yaml` or `tasks.json` configuration files.

## How It Works

1. **Setup**: AutoFocus reads the files generated by AutoRecon, which are organised by IP address directories, and processes them recursively to analyse the reconnaissance data.

2. **Data Chunking**: Files are processed in defined chunks to ensure that the input to the LLM is manageable, minimising processing load and maintaining efficiency.

3. **Task Execution**: Each chunk is analysed according to a list of configurable tasks. The LLM identifies version numbers, points of interest, and vulnerabilities based on the defined analysis tasks.

4. **Results and Output**: Results for each task are saved in a structured JSON format, organised by IP address and task type. This output format makes it easy to review findings, generate reports, or integrate results into further testing processes.

## Getting Started

### Prerequisites

- Python 3.8 or later
- A local large language model (e.g., Qwen 2.5) to enable offline processing using Ollama
- AutoRecon for initial data collection

### Installation

Clone the repository:
```bash
git clone https://github.com/LiterallyBlah/AutoFocus.git
cd AutoFocus
```

Install the required Python packages:
```bash
pip install -r requirements.txt
```

### Usage

1. Ensure AutoRecon has generated the necessary directories for each target IP.

2. Run AutoFocus:
   ```bash
   python autofocus.py --input /path/to/autorecon/output --output /path/to/output.json
   ```

3. Configure analysis tasks by editing the `tasks.yaml` (or `tasks.json`) file, where you can define the tasks you want AutoFocus to perform.

### Example Command

```bash
python autofocus.py --input /path/to/recon/data --output /path/to/output --tasks /path/to/tasks.yml
```

This command processes the reconnaissance data from the specified input directory and saves the results in the output directory, performing the tasks specified in the tasks file.

## Task Configuration

Tasks are specified in a YAML file, and each task requires careful configuration and prompt engineering to achieve optimal results:

- **name**: A unique identifier for the task.
- **description**: A brief description of what the task checks for. This should be clear and specific to guide the LLM's focus.
- **response**: Instructions for the LLM on what to return (e.g., version numbers, points of interest, vulnerabilities). These instructions need to be fine-tuned to elicit precise and relevant responses.
- **output**: The structure of the result in the final JSON output.

Fine-tuning and prompt engineering are crucial for each task:

1. **Precision**: Craft prompts that encourage the LLM to provide specific, targeted information.
2. **Consistency**: Ensure prompts maintain a consistent format across tasks for easier processing.
3. **Context**: Provide enough context in the description to guide the LLM's understanding of the task's purpose.
4. **Iterative Improvement**: Regularly review and refine prompts based on the quality of results obtained.
5. **Avoid Ambiguity**: Use clear, unambiguous language to prevent misinterpretation by the LLM.

Example (`tasks.yaml`):
```yaml
tasks:
  - name: "version_check"
    description: "Identify software version numbers for correlation with known issues. Ensure that there is a software and version number."
    response: "Only return the software name and version numbers."
    output: "version_numbers"
  - name: "points_of_interest"
    description: "Highlight unusual or potentially risky configurations."
    response: "Only return POI if there is something unusual or risky."
    output: "poi"
  - name: "vulnerability_scan"
    description: "Extract and list all vulnerabilities found."
    response: "Only return vulnerabilities."
    output: "vulnerabilities"
```

## Contribution

Contributions are welcome! If you have ideas for improvements, new features, or bug fixes, feel free to open a pull request or raise an issue.

## Licence

This project is licensed under the MIT Licence.

